# 优化网卡中断
首先，要对软中断有一个认识，程序运行后，操作系统会发送程序需要的一些cpu指令到某个cpu，扔给CPU的这个过程是异步的，cpu获得指令后操作完成会触发一个硬中断，并且把操作的结果保存在寄存器，之后linux内核会启动ksofttrip进程去，来获取操作结果，这个动作就叫做软中断。
linux默认会起n个ksofttrip进程，n等于cpu的个数，ksofttrip是死循环，只要有软中断，它就会一直去获取，n个ksoftrip获取源是一样的，为什么要起n个进程呢？就是为了 ，当某个cpu空闲，哪个就去跑。通常操作系统里它的进程名是 ksoftrip/n  ,n是对应的cpu的编号，ksoft进程跟cpu是一对一绑定的。
现在来说说网卡的性能问题，要想优化，首先你的网卡必须是多通道队列的。那如何知道你的网卡是否是多队列的呢？  

通过cat /proc/interrept |grep eth0|wc -l  可以看到网卡通道队列的数量.

现在来来说说优化方案，为什么要优化，因为linux默认情况所有的网卡的软中断都是的cpu0，所以加入你的ksoftrip/0总是跑满,就说明可能是网卡问题了。

## 方案1 ，SMP IRQ affinity技术
说白了，就是信号量分布技术，把特定信号量的处理放到固定的cpu上，每个网卡的通道队列都有一个自己的信号量。
首先查看所有网卡通道队列的信号量，方法 
```
cat/proc/interrept |grep eth0 
```
每行最开头的数字“n:”就是信号量，在/proc/irq/下面可以找到对应的以信号量命名的目录找完了之后，可以进行信号量绑定了，在/proc/irq/n/下面有两个文件，分别是smp_affinity跟smp_affinity_list,  这两个是文件的内容是对应的，smp_affinity里是通过bitmask算法绑定cpu，smp_affinity_list是通过数字指定cpu编号的方法，例如 cpu0，文件里就是“0”,如果是cpu1跟2就是“1,2”
！！重点来了，虽然默认里面填写的是多个，但是！！！但是它只跑在绑定cpu中的第一个！！！坑啊！！！
所以，你要做的就是单独绑定每一个网卡的通道队列。直接：

```
  echo "1" >/proc/irq/$(cpu0的信号量)/snmp_affinity_list
  echo "2" >/proc/irq/$(cpu1的信号量)/snmp_affinity_list
  echo "3" >/proc/irq/$(cpu2的信号量)/snmp_affinity_list
```

这个是最快速的解决方案，提升效率显著啊！！！

## 升级方案2，在方案1基础之上，RPS/RFS技术
此技术大家可以查网上，文章很多，优化效果是，单个网卡通道队列的软中断会平均到所有cpu上，并且会优化为，中断落在发出中断的程序所在的那个cpu上，这样节省了cpu cache。

坏消息是对单队列网卡而言，「smp_affinity」和「smp_affinity_list」配置多CPU无效。

好消息是Linux支持RPS，通俗点来说就是在软件层面模拟实现硬件的多队列网卡功能。

首先看看如何配置RPS，如果CPU个数是 8 个的话，可以设置成 ff：

```
  shell> echo ff > /sys/class/net/eth0/queues/rx-0/rps_cpus
```

接着配置内核参数rps_sock_flow_entries（官方文档推荐设置： 32768）：
```

  shell> sysctl net.core.rps_sock_flow_entries=32768
```

最后配置rps_flow_cnt，单队列网卡的话设置成rps_sock_flow_entries即可：
```

  echo 32768 > /sys/class/net/eth0/queues/rx-0/rps_flow_cnt
```

说明：如果是多队列网卡，那么就按照队列数量设置成 rps_sock_flow_entries / N 。
转自：[关于linux 软中断对网卡性能的影响以及优化](https://www.douban.com/note/509842794/?type=like)
